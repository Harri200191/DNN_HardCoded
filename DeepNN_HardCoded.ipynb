{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import time\n",
    "import h5py\n",
    "import scipy\n",
    "\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "sns.set()\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):    \n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters     \n",
    "\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    else:\n",
    "        print(\"\\033[91mError! Please make sure you have passed the value correctly in the \\\"activation\\\" parameter\")\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)     \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    else:\n",
    "        print(\"\\033[91mError! Please make sure you have passed the value correctly in the \\\"activation\\\" parameter\")\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p\n",
    "\n",
    "def print_mislabeled_images(classes, X, y, p):\n",
    "    a = p + y\n",
    "    mislabeled_indices = np.asarray(np.where(a == 1))\n",
    "    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n",
    "    num_images = len(mislabeled_indices[0])\n",
    "    for i in range(num_images):\n",
    "        index = mislabeled_indices[1][i]\n",
    "        \n",
    "        plt.subplot(2, num_images, i + 1)\n",
    "        plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Prediction: \" + classes[int(p[0,index])].decode(\"utf-8\") + \" \\n Class: \" + classes[y[0,index]].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_binary():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    train_filter = np.logical_or(y_train == 0, y_train == 1)\n",
    "    test_filter = np.logical_or(y_test == 0, y_test == 1)\n",
    "\n",
    "    x_train_binary = x_train[train_filter]\n",
    "    y_train_binary = y_train[train_filter]\n",
    "\n",
    "    x_test_binary = x_test[test_filter]\n",
    "    y_test_binary = y_test[test_filter]\n",
    "\n",
    "    return (x_train_binary, y_train_binary), (x_test_binary, y_test_binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_file(zip_path, extract_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = \"C:\\\\Users\\\\haris\\\\Downloads\\\\New folder\\\\Files.zip\"\n",
    "extract_path = \"C:\\\\Users\\\\haris\\\\Downloads\\\\New folder\\\\\"\n",
    "unzip_file(zip_path, extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (12665, 28, 28)\n",
      "y_train shape: (12665, 1)\n",
      "x_test shape: (2115, 28, 28)\n",
      "y_test shape: (2115, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_mnist_binary()\n",
    "print('x_train shape:', x_train.shape)\n",
    "y_train = np.reshape(y_train, [-1, 1])\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_test = np.reshape(y_test, [-1, 1])\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a [1] picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 10\n",
    "plt.imshow(x_train[index], cmap = \"gray\")\n",
    "print (\"It's a \" + str(y_train[index]) +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (784, 12665)\n",
      "test_x's shape: (784, 2115)\n"
     ]
    }
   ],
   "source": [
    "train_x_flatten = x_train.reshape(x_train.shape[0], -1).T\n",
    "test_x_flatten = x_test.reshape(x_test.shape[0], -1).T\n",
    "\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 784    \n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)\n",
    "learning_rate = 0.0075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [784, 20, 7, 5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []                       \n",
    "    parameters = initialize_parameters_deep(layers_dims)   \n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)        \n",
    "\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (12665,1) and (12665,1) not aligned: 1 (dim 1) != 12665 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m parameters, costs \u001b[39m=\u001b[39m L_layer_model(train_x, y_train, layers_dims, num_iterations \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, print_cost \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCost after first iteration: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(costs[\u001b[39m0\u001b[39m]))\n\u001b[0;32m      5\u001b[0m L_layer_model_test(L_layer_model)\n",
      "Cell \u001b[1;32mIn[66], line 8\u001b[0m, in \u001b[0;36mL_layer_model\u001b[1;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, num_iterations):\n\u001b[0;32m      7\u001b[0m     AL, caches \u001b[39m=\u001b[39m L_model_forward(X, parameters)\n\u001b[1;32m----> 8\u001b[0m     cost \u001b[39m=\u001b[39m compute_cost(AL, Y)\n\u001b[0;32m     10\u001b[0m     grads \u001b[39m=\u001b[39m L_model_backward(AL, Y, caches)\n\u001b[0;32m     11\u001b[0m     parameters \u001b[39m=\u001b[39m update_parameters(parameters, grads, learning_rate)        \n",
      "Cell \u001b[1;32mIn[56], line 85\u001b[0m, in \u001b[0;36mcompute_cost\u001b[1;34m(AL, Y)\u001b[0m\n\u001b[0;32m     82\u001b[0m m \u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m     84\u001b[0m \u001b[39m# Compute loss from aL and y.\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m cost \u001b[39m=\u001b[39m (\u001b[39m1.\u001b[39m\u001b[39m/\u001b[39mm) \u001b[39m*\u001b[39m (\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39;49mdot(Y,np\u001b[39m.\u001b[39;49mlog(AL)\u001b[39m.\u001b[39;49mT) \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mY, np\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mAL)\u001b[39m.\u001b[39mT))\n\u001b[0;32m     87\u001b[0m cost \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqueeze(cost)      \u001b[39m# To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[39massert\u001b[39;00m(cost\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m ())\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (12665,1) and (12665,1) not aligned: 1 (dim 1) != 12665 (dim 0)"
     ]
    }
   ],
   "source": [
    "parameters, costs = L_layer_model(train_x, y_train, layers_dims, num_iterations = 1, print_cost = False)\n",
    "\n",
    "print(\"Cost after first iteration: \" + str(costs[0]))\n",
    "\n",
    "L_layer_model_test(L_layer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
